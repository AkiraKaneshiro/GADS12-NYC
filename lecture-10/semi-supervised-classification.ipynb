{
 "metadata": {
  "name": "semi-supervised-classification"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clustering Features\n",
      "\n",
      "In this lab we build a semi-supervised classifier that can predict whether a photograph depicts a cat or a dog.  \n",
      "\n",
      "Classifying natural images is a challenging problem. We will extract a variable number of SURF descriptors from each image.  \n",
      "Recall that SURF descriptors describe \"interesting\" regions in images.\n",
      "We then cluster the SURF descriptors, and train the classifier using the clusters as features.  \n",
      "\n",
      "We require the `mahotas` library to cluster the images. Install `mahotas` by executing `pip install mahotas` in your terminal emulator."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First we will make a list of the files that need to be loaded and create a list of their classes.\n",
      "import glob\n",
      "all_instance_filenames = []\n",
      "all_instance_targets = []\n",
      "for f in glob.glob('cats-and-dogs-img/*.jpg'):\n",
      "    target = 1 if 'cat' in f[18:] else 0\n",
      "    all_instance_filenames.append(f)\n",
      "    all_instance_targets.append(target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We will extract SURF descriptors from each of the images.\n",
      "import mahotas as mh\n",
      "from mahotas.features import surf\n",
      "surf_features = []\n",
      "counter = 0\n",
      "for f in all_instance_filenames:\n",
      "    if counter % 100 == 0:\n",
      "        print 'Loaded image %s of 2000' % counter\n",
      "    counter += 1\n",
      "    image = mh.imread(f, as_grey=True)\n",
      "    surf_features.append(surf.surf(image)[:, 5:])\n",
      "print 'SURF extraction complete.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded image 0 of 2000\n",
        "Loaded image 100 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 200 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 300 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 400 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 500 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 600 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 700 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 800 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 900 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1000 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1100 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1200 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1300 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1400 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1500 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1600 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1700 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1800 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Loaded image 1900 of 2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "1999"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "surf_features[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "surf_features[1].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import *\n",
      "from sklearn.cluster import MiniBatchKMeans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_len = int(len(all_instance_filenames) * .60)\n",
      "X_train_surf_features = np.concatenate(surf_features[:train_len])\n",
      "X_test_surf_feautres = np.concatenate(surf_features[train_len:])\n",
      "y_train = all_instance_targets[:train_len]\n",
      "y_test = all_instance_targets[train_len:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_clusters = 150\n",
      "print 'Clustering', len(X_train_surf_features), 'features'\n",
      "estimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
      "estimator.fit_transform(X_train_surf_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print surf_features[231]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = []\n",
      "for instance in surf_features[:train_len]:\n",
      "    if len(instance) == 0:\n",
      "        X_train.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_train.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = []\n",
      "for instance in surf_features[train_len:]:\n",
      "    if len(instance) == 0:\n",
      "        X_train.append(np.zeros(150))\n",
      "        continue\n",
      "    clusters = estimator.predict(instance)\n",
      "    features = np.bincount(clusters)\n",
      "    if len(features) < n_clusters:\n",
      "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "    X_test.append(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(X_train)\n",
      "print len(X_train)\n",
      "print X_train[0].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = LogisticRegression()\n",
      "clf.fit_transform(X_train, y_train)\n",
      "predictions = clf.predict(X_test)\n",
      "print classification_report(y_test, predictions)\n",
      "print 'Precision:', precision_score(y_test, predictions)\n",
      "print 'Recall:', recall_score(y_test, predictions)\n",
      "print 'Accuracy:', accuracy_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}