{
 "metadata": {
  "name": "",
  "signature": "sha256:7a7be88166f361cdccc6fc1df599554a81b63015cc7cfc7c3175cf6f7e5c406c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Document Classification Lab"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basic Text Feature Extraction with the Bag-of-Words Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's create feature representations for the following corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "    'UNC played Duke in basketball',\n",
      "    'Duke lost the basketball game'\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corpus is represented as a `List` of documents. The documents are represented as `String`s. The bag-of-words representation requires operating on individual words. We must *tokenize* the documents. A token is a meaningful, atomic unit of text. Tokens are often words, but can also be affixes, punctuation, etc.  \n",
      "Our documents only contains words, so we can tokenize the documents by splitting the `String`s on whitespace. The `split(delimiter)` method of `String`s can be used to tokenize documents on spaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a `Set` of the words in the corpus.\n",
      "vocabulary_1 = set(corpus[0].split(' ') + corpus[1].split(' '))\n",
      "print vocabulary_1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['Duke', 'basketball', 'lost', 'played', 'game', 'UNC', 'in', 'the'])\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's map the elements of the `Set` to indices, and create feature representations for our corpus by iterating through the tokens in each document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO create a dictionary that maps each token in the vocabulary to an index\n",
      "dictionary_1 = {token: i for (i, token) in enumerate(vocabulary_1)}\n",
      "# TODO Now create a feature representation for each document in the corpus.\n",
      "# The feature representations can be lists of integers.\n",
      "X = []\n",
      "for sentence in corpus:\n",
      "    x = [0] * len(vocabulary_1)\n",
      "    for token in sentence.split(' '):\n",
      "        x[dictionary_1[token]] = 1\n",
      "    X.append(x)\n",
      "# TODO print the feature representations\n",
      "print X\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1, 1, 0, 1, 0, 1, 1, 0], [1, 1, 1, 0, 1, 0, 0, 1]]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the CountVectorizer transformer from sklearn.feature_extraction.text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "# Create an instance of CountVectorizer. Set the keyword argument `binary` to True.\n",
      "# binary_vectorizer = \n",
      "binary_vectorizer = CountVectorizer(binary=True)\n",
      "# Fit the vectorizer on the corpus, and transform the corpus\n",
      "# X = \n",
      "X = binary_vectorizer.fit_transform(corpus)\n",
      "# TODO Print the vectorizer's vocabulary. Is the collection the same as the vocabulary you created?\n",
      "# TODO Print the transformed feature representations. You can print the dense matrix using the method `todense()`.\n",
      "# The vectors may not be the same as the elements are not ordered as the words were encountered.\n",
      "print X.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1 0 1 0 1 0 1]\n",
        " [1 1 1 0 1 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectors that have many zero-valued elements are called *sparse vectors*. Sparse matrices can be represented efficiently by storing only the indices and values of non-zero elements.  \n",
      "`CountVectorizer` returns sparse matrices by default. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO now add a third document, \"I ate a sandwich,\" to the corpus.\n",
      "# corpus\n",
      "corpus.append('I ate a sandwich')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO fit the vectorizer again and print the vocabulary. What words should you expect to be added?\n",
      "binary_vectorizer.fit(corpus)\n",
      "print binary_vectorizer.vocabulary_\n",
      "# TODO are any words missing? (Spoiler: yes). \n",
      "# Consult the documentation for the class to learn why: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
      "# TODO transform the corpus and print the feature representations again.\n",
      "X = binary_vectorizer.transform(corpus)\n",
      "print binary_vectorizer.transform(corpus).todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n",
        "[[0 1 1 0 1 0 1 0 0 1]\n",
        " [0 1 1 1 0 1 0 0 1 0]\n",
        " [1 0 0 0 0 0 0 1 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is the first document more similar to the second or third documents? Do the feature representations encode this similarity?  \n",
      "\n",
      "Recall that the L^2 norm of a vector is given by the following equation: \n",
      "\n",
      "$$||x||_2 = \\sqrt{x_1^2 + x_2^2 , \\dotsc, x_n^2}$$\n",
      "\n",
      "Let's compare the documents in the corpus using their distances from each other in space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "for i in range(0, len(X.todense())):\n",
      "    for j in range(i+1, len(X.todense())):\n",
      "        print 'The Euclidean distance between [%s] and [%s] is [%s]' % (corpus[i], corpus[j], euclidean_distances(X[i], X[j]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Euclidean distance between [UNC played Duke in basketball] and [Duke lost the basketball game] is [[[ 2.44948974]]]\n",
        "The Euclidean distance between [UNC played Duke in basketball] and [I ate a sandwich] is [[[ 2.64575131]]]\n",
        "The Euclidean distance between [Duke lost the basketball game] and [I ate a sandwich] is [[[ 2.64575131]]]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first two documents pertain to college basketball. The third document does not. Accordingly, the first two documents are closer to each other than they are to the third document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO add two documents to the corpus. \n",
      "# One document should be more similar to the first document than the others, \n",
      "# and the other document should be more similar to the third document than the others."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Document Classification\n",
      "\n",
      "Let's use our bag-of-words feature representations with a logistic regressor to classify documents. We will build a contemporary version of the canonical supervised machine learning application, the spam classifier. Instead of classifying email, we will classify SMS messages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loading and Inspecting the Data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Import numpy as np, pandas as pd, and pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# TODO read `sms.csv` into a `DataFrame`.\n",
      "df = pd.read_csv('sms.csv')\n",
      "# TODO what are the dataframe's columns?\n",
      "print df.columns.values\n",
      "# TODO are the classes balanced? Use the `value_counts()` method to count the number of spam and ham messages.\n",
      "print df['label'].value_counts()\n",
      "# TODO inspect the data. Print the first 10 spam and ham messages\n",
      "print df['message'][df['label'] == 1].head(10)\n",
      "print df['message'][df['label'] == 0].head(10)\n",
      "# TODO assign the 'label' column to y, and the 'message' column to X.\n",
      "y_raw = df['label']\n",
      "X_raw = df['message']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['label' 'message']\n",
        "0    4827\n",
        "1     747\n",
        "dtype: int64\n",
        "2     Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "5     FreeMsg Hey there darling it's been 3 week's n...\n",
        "8     WINNER!! As a valued network customer you have...\n",
        "9     Had your mobile 11 months or more? U R entitle...\n",
        "11    SIX chances to win CASH! From 100 to 20,000 po...\n",
        "12    URGENT! You have won a 1 week FREE membership ...\n",
        "15    XXXMobileMovieClub: To use your credit, click ...\n",
        "19    England v Macedonia - dont miss the goals/team...\n",
        "34    Thanks for your subscription to Ringtone UK yo...\n",
        "42    07732584351 - Rodger Burns - MSG = We tried to...\n",
        "Name: message, dtype: object\n",
        "0     Go until jurong point, crazy.. Available only ...\n",
        "1                         Ok lar... Joking wif u oni...\n",
        "3     U dun say so early hor... U c already then say...\n",
        "4     Nah I don't think he goes to usf, he lives aro...\n",
        "6     Even my brother is not like to speak with me. ...\n",
        "7     As per your request 'Melle Melle (Oru Minnamin...\n",
        "10    I'm gonna be home soon and i don't want to tal...\n",
        "13    I've been searching for the right words to tha...\n",
        "14                  I HAVE A DATE ON SUNDAY WITH WILL!!\n",
        "16                           Oh k...i'm watching here:)\n",
        "Name: message, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Partitioning and Transforming the Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `train_test_split` convenience function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import train_test_split\n",
      "# TODO split the data set into training and testing sets\n",
      "# X_train, X_test, y_train, y_test = \n",
      "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw)\n",
      "\n",
      "# TODO fit the `binary_vectorizer` on the training set\n",
      "X_train = binary_vectorizer.fit_transform(X_train)\n",
      "# TODO how many words are in the vocabulary?\n",
      "print len(binary_vectorizer.vocabulary_)\n",
      "# TODO transform the training set\n",
      "# TODO transform the test set\n",
      "X_test = binary_vectorizer.transform(X_test)\n",
      "# TODO print X_train\n",
      "print X_train.todense(), X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7503\n",
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (4180, 7503)\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training the Model "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `LogisticRegression` class from the `linear_model` module.\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "# TODO create an instance of `LogisticRegression`\n",
      "# classifier = \n",
      "classifier = LogisticRegression()\n",
      "# TODO fit the classifier on the training data\n",
      "classifier.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluating the Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO evaluate the classifier. What is the default performance measure for `LogisticRegression`? \n",
      "# (Hint: consult the documentation)\n",
      "print 'accuracy', classifier.score(X_test, y_test)\n",
      "# Let's evaluate the classifier using other performance measures.\n",
      "# TODO import the `recall_score`, `precision_score`, and `f1_score` functions from the `metrics` module.\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "# TODO make predictions for the test set\n",
      "# predictions = \n",
      "predictions = classifier.predict(X_test)\n",
      "# TODO evaluate the classifier's precision, recall, and F1 score.\n",
      "print 'recall', recall_score(y_test, predictions)\n",
      "print 'precision', precision_score(y_test, predictions)\n",
      "print 'f1', f1_score(y_test, predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy 0.982783357245\n",
        "recall 0.880208333333\n",
        "precision 0.994117647059\n",
        "f1 0.933701657459\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluating the classifier on a single training and test partition doesn't provide the best estimate of its performance. What if the class proportions are imbalanced, and all of the instances of one class appear only in one of the partitions?  \n",
      "Instead, we will evaluate the model using *cross validation*. We will make `n` partitions. We will train on `n-1` partitions and evaluate on the nth partition. We will then rotate the partitions, retrain, and re-test. \n",
      "We will continue rotating the partitions until we have trained and evaluated using all of the partitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the `cross_val_score` function from the `cross_validation` module.\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "# `cross_val_score()` takes the estimator object, the design matrix, and the values of the response variable.\n",
      "# It also takes the `cv` keyword argument to set the number of cross validation partitions.\n",
      "# `cross_val_score()` will train and test with each partition.\n",
      "# This means that we cannot fit the `CountVectorizer` once; we must fit it for each cross validation fold.\n",
      "# We can combine the `CountVectorizer` and the classifier to form a `Pipeline`. We can then pass the `Pipeline` to `cross_val_score()`.\n",
      "from sklearn.pipeline import Pipeline\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(binary=True)),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "print accuracy_scores, np.mean(accuracy_scores)\n",
      "# TODO now evaluate the classifier's `recall`, `precision`, and `F1` score using cross validation.\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print precision_scores, np.mean(precision_scores)\n",
      "print recall_scores, np.mean(recall_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.98476703  0.98566308  0.98114901  0.98294434  0.98294434] 0.9834935619\n",
        "[ 1.          1.          0.99230769  1.          0.99242424]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.996946386946\n",
        "[ 0.88666667  0.89333333  0.86577181  0.87248322  0.87919463] 0.879489932886\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Text Feature Dimensionality Reduction\n",
      "\n",
      "Recall that the *curse of dimensionality* refers to a collection of problems encountered when using high-dimensional data. A large corpus can easily contain hundreds of thousands of unique tokens. Let's examine some techniques for reducing the dimensions of text features.\n",
      "\n",
      "#### Lower-casing\n",
      "\n",
      "A basic strategy for reducing the dimensions of the feature space is to convert all of the text to lowercase. This is motivated by the intuition that the letter case does not contribute to the meanings of most words: \"sandwich\" and \"Sandwich\" have the same meaning in most contexts. Capitalization may indicate that a word is beginning a sentence, but the bag-of-words model has already discarded all information from word order and grammar.\n",
      "\n",
      "#### Stop Word Filtering\n",
      "\n",
      "A second strategy is to remove words that are common to most of the documents in the corpus. These words, called stop words, frequently include determiners like \"the\" and \"an\", auxiliary verbs like \"do\" and \"has\", and prepositions, like \"on\" and \"beneath\". Stop words are often functional words that contribute to the document's meaning through grammar rather than their denotations. \n",
      "\n",
      "CountVectorizer can filter stop words provided as the `stop_words` keyword parameter, and also includes a basic English stop list. Let's re-create the feature vectors for our documents using lower-casing and stop filtering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "    'The sandwich was eaten by me.',\n",
      "    'I ate a sandwich, you ate a SANDWICH, and the cat ate a sandwich.'\n",
      "]\n",
      "# TODO Create an instance of `CountVectorizer`. Set the `stop_words` keyword argument to 'english'.\n",
      "tf_vectorizer = CountVectorizer(stop_words='english')\n",
      "# TODO fit and transform the corpus.\n",
      "X_toy = tf_vectorizer.fit_transform(corpus)\n",
      "print tf_vectorizer.vocabulary_\n",
      "# TODO print the features and the vocabulary. What happened?\n",
      "print X_toy.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'sandwich': 3, u'ate': 0, u'eaten': 2, u'cat': 1}\n",
        "[[0 0 1 1]\n",
        " [3 1 0 3]]\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Stemming and Lemmatization\n",
      "\n",
      "While stop filtering is an easy strategy for dimensionality reduction, most stop lists contain only a few hundred words. A large corpus may still have hundreds of thousands of unique words after filtering. Two similar strategies for further reducing dimensionality are called stemming and lemmatization.  \n",
      "\n",
      "A high-dimensional document vector may separately encode several derived or inflected forms of the same word. For example, \"jumping\" and \"jumps\" are both forms of the word \"jump\"; a document vector in a corpus of long-jumping articles may encode each inflected form with a separate element in the feature vector.  Stemming and lemmatization are two strategies for condensing inflected and derived forms of a word into a single feature."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Consider the following corpus:\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "corpus = [\n",
      "    'He ate the sandwiches',\n",
      "    'Every sandwich was eaten by him'\n",
      "]\n",
      "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
      "print vectorizer.fit_transform(corpus).todense()\n",
      "print vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 0 1]\n",
        " [0 1 1 0]]\n",
        "{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The documents have similar meanings, but their feature vectors have no elements in common! Both documents contain a conjugation of \u201cate\u201d and an inflected form of \u201csandwich\u201d; ideally, these similarities should be reflected in the feature vectors.  \n",
      "\n",
      "*Lemmatization* is the process of determining the lemma, or the morphological root of an inflected word based on its context. Lemmas are the base forms of words that index the word in the dictionary.  \n",
      "\n",
      "*Stemming* has a similar goal to lemmatization, but it does not attempt to produce the morphological roots of words. Instead, stemming removes all patterns of characters that appear to be affixes, resulting in a token that is not necessarily a valid word. Lemmatization frequently requires a lexical resource, like WordNet, and the word's part-of-speech. Stemming algorithms frequently use rules instead of lexical resources to produce stems, and can operate on any token, even without its context."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO if you do not have NLTK, you can install it using `pip install nltk`.\n",
      "# TODO import the functions `word_tokenize` and `pos_tag` from the module `nltk`.\n",
      "from nltk import word_tokenize, pos_tag\n",
      "# TODO import the class `PorterStemmer` from NLTK's `stem` module.\n",
      "from nltk.stem import PorterStemmer\n",
      "# TODO import the class `WordNetLemmatizer` from `nltk.stem.wordnet`.\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "\n",
      "wordnet_tags = ['n', 'v']\n",
      "corpus = [\n",
      "    'He ate the sandwiches',\n",
      "    'Every sandwich was eaten by him'\n",
      "]\n",
      "\n",
      "# TODO instantiate a `PorterStemmer`\n",
      "# stemmer = \n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "# TODO use `word_tokenize` to token each document in the corpus. Then stem and print each token.\n",
      "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]\n",
      "\n",
      "# TODO instantiate a WordNetLemmatizer.\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "# The WordNetLemmatizer requires the token and its corresponding part-of-speech.\n",
      "# For a complete list of the Penn Treebank POS tags see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
      "# The following function takes a token and its POS tag and returns the lemma or None.\n",
      "def lemmatize(token, tag):\n",
      "    if tag[0].lower() in ['n', 'v']:\n",
      "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
      "    return token\n",
      "\n",
      "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
      "print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
        "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Improving the Bag-of-Words Representation\n",
      "\n",
      "### Term Frequency Weights\n",
      "\n",
      "Let's see if we can improve our model's performance. Currently we represent whether or not the token is present in the document. It is intuitive that the frequency with which a token appears in a text is indicative of \"how much\" the document pertains to that word. A document that contains a word once may be about a different topic than a document that contains the same word dozens of times. Let's represent our documents using raw term frequencies instead of binary frequencies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's examine the term frequencies produced for a toy corpus.\n",
      "corpus = [\n",
      "    'I ate some pizza.',\n",
      "    'I ate a sandwich, you ate a sandwich, and the cat ate a sandwich.'\n",
      "]\n",
      "# TODO create a new instance of CountVectorizer. Do not pass the keyword argument `binary=True`. \n",
      "# It will default to false, and return raw term frequencies.\n",
      "# tf_vectorizer =\n",
      "tf_vectorizer = CountVectorizer()\n",
      "# TODO fit `tf_vectorizer` on the corpus, and transform the corpus\n",
      "# X_toy = \n",
      "X_toy = tf_vectorizer.fit_transform(corpus)\n",
      "# TODO print `tf_vectorizer`'s vocabulary and the transformed features.\n",
      "print X_toy.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 1 0 1 0 1 0 0]\n",
        " [1 3 1 0 3 0 1 1]]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's compare the performance of the classifier using the binary and TF features.\n",
      "# TODO create a new `Pipeline` that uses a `CountVectorizer` without the `binary=True` constructor argument.\n",
      "# pipeline = \n",
      "pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(stop_words='english')),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "# TODO evaluate the accuracy, precision, and recall using cross validation.\n",
      "# accuracy_scores = \n",
      "# precision_scores = \n",
      "# recall_scores = \n",
      "# TODO print the scores and their means. Did using term frequency weights help?\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print accuracy_scores, np.mean(accuracy_scores)\n",
      "print precision_scores, np.mean(precision_scores)\n",
      "print recall_scores, np.mean(recall_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.98207885  0.9811828   0.97396768  0.97666068  0.98114901] 0.979007805512\n",
        "[ 1.          0.99236641  0.99180328  1.          1.        ] 0.99683393818\n",
        "[ 0.86666667  0.86666667  0.81208054  0.82550336  0.8590604 ] 0.845995525727\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the classifier performs slightly worse with term frequency weights than binary features. These documents are text messages; term frequencies should be expected to perform better than binary features for longer documents."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Normalized Term Frequency Weights\n",
      "\n",
      "Encoding the terms' raw frequencies in the feature vector provides additional information about the meanings of the documents, but assumes that all of the documents are of similar lengths. Many words might appear with the same frequency in two documents, but the documents could still be dissimilar if one document is many times larger than the other.  \n",
      "\n",
      "scikit-learn's `TfdfTransformer` can mitigate this problem by transforming a matrix of term frequency weights into a matrix of normalized term frequency weights. By default, `TfdfTransformer` smooths the raw counts and applies L2 normalization. The smoothed, normalized term frequencies are given by the equation:\n",
      "\n",
      "$tf(t, f) = \\frac{f(t, d)}{||x||}$\n",
      "\n",
      "### Inverse Document Weights\n",
      "\n",
      "While normalized term frequencies mitigate the effects of different document lengths, another problem remains. Our feature vectors contain large weights for terms that occur frequently in a document, even if those terms occur frequently in most documents in the corpus. These terms do not help to represent the meaning of a particular document relative to the rest of the corpus. For example, most of the documents in a corpus of articles about Duke's basketball team could include the words \u201cbasketball\u201d, \u201cCoach K\u201d, and \u201cflop\u201d. These words can be thought of as corpus-specific stop words, and may not be useful for calculating the similarity of documents. The *inverse document frequency*, or IDF, is a measure of how rare or common a word is in a corpus. The inverse document frequency is given by the equation:\n",
      "\n",
      "$idf(t, d) = log \\frac{N}{1 + | d \\in D: t \\in d|}$\n",
      "\n",
      "where $N$ is the total number of documents in the corpus, and $| d \\in D: t \\in d|$ is the number of documents in the corpus that contain term . A term's tf-idf weight is the product of its term frequency and inverse document frequency. `TfidfTransformer` returns tf-idfs weight when its `use_idf` keyword argument is set to its default value, `True`. Since tf-idf weighted feature vectors are commonly used to represent text, scikit-learn provides a `TfidfVectorizer` class that wraps `CountVectorizer` and `TfidfTransformer`. Let's use `TfidfVectorizer` to create tf-idf weighted feature vectors for our corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
      "categories = ['alt.atheism', 'rec.sport.hockey']\n",
      "newsgroups = fetch_20newsgroups(categories=categories)\n",
      "\n",
      "X_raw = newsgroups.data\n",
      "y_raw = newsgroups.target\n",
      "\n",
      "pipeline = Pipeline([\n",
      "                     #('vect', TfidfVectorizer(stop_words='english')),\n",
      "                     ('vect', CountVectorizer(binary=True, stop_words='english')),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "# TODO evaluate the accuracy, precision, and recall using cross validation.\n",
      "# accuracy_scores = \n",
      "# precision_scores = \n",
      "# recall_scores = \n",
      "# TODO print the scores and their means. Did using term frequency weights help?\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print accuracy_scores, np.mean(accuracy_scores)\n",
      "print precision_scores, np.mean(precision_scores)\n",
      "print recall_scores, np.mean(recall_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.          0.99537037  1.          0.99537037  1.        ] 0.998148148148\n",
        "[ 1.          0.99173554  1.          0.99173554  1.        ] 0.996694214876\n",
        "[ 1.  1.  1.  1.  1.] 1.0\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO reevaluate"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}