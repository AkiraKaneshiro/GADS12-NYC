{
 "metadata": {
  "name": "mlp_mnist"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Hand-Written Digit Classification with Multilayer Perceptrons\n",
      "\n",
      "![Digits](http://www.heikohoffmann.de/htmlthesis/img679.gif)  \n",
      "\n",
      "In this lab we will build a multilayer perceptron that can classify hand-written digits. The subset of MNIST included with scikit-learn contains 1,797 of the dataset's 70,000 images. Each image in the scikit-learn version is 8x8 pixels in dimension, grayscale, and depicts one of the digits 0 through 9. The digits were written by US high school students and employees of the Census Bureau. The MNIST dataset is a popular benchmark for classifiers; you can examine the performances of different models at [this page](http://yann.lecun.com/exdb/mnist/). (Note that the error rates are expressed as percentages; scikit-learn would report the accuracy for the state-of-the-art neural network as 1.0-0.0023=0.9977, or almost perfect. This problem is fairly easy--the accuracy of a KNN model is only a few points less--so it does not fully demonstrate the power of artificial neural networks. The problems that do demonstrate the potential of ANNs, however, are not appropriate for our labs). \n",
      "\n",
      "We will build a network with an input layer with 64 units, two hidden layers with 150 and 100 units, respectively, and an output layer with one unit for each of the 10 possible classes. The ten output units will return the probability that the test instance is the corresponding class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we will use the `load_digits` convenience function to load the MNIST dataset. We will fork additional processes during cross validation, which requires execution from a main-protected block in normal Python scripts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the load_digits convenience function from sklearn.datasets\n",
      "\n",
      "# TODO create an instance of the data bunch\n",
      "\n",
      "# TODO assign the bunch's `data` attribute to X\n",
      "\n",
      "# TODO assign the bunch's `target` attribute to y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO how many instances do we have? How many feaures are we using?\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scaling the features is particularly important for artificial neural networks and will help some learning algorithms to converge more quickly. Next, we will create a `Pipeline` that scales the data before fitting a `MultilayerPerceptronClassifier`. This network contains an input layer, a hidden layer with 150 units, a hidden layer with 100 units, and an output layer. The input and output layers are created automatically based on the number of classes and the number of features. We also will set the value of the regularization hyperparameter `alpha` to 0.1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import Pipeline\n",
      "\n",
      "# TODO import the StandardScaler transformer from the preprcoessing module\n",
      "\n",
      "# TODO import MultilayerPerceptronClassifier from the neural_network module\n",
      "\n",
      "# TODO create a Pipeline with a StandardScaler and a MultilayerPerceptronClassifier.\n",
      "# TODO Define the architecture of the network using the n_hidden keyword argument, \n",
      "# which accepts a list of the number of units in each hidden layer.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Now we will print the accuracies of the three cross validation folds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import cross_val_score from the cross_validation module\n",
      "\n",
      "# TODO print the scores for three-fold cross validation\n",
      "\n",
      "# What performance metric are we using?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO make predictions and print the confusion matrix\n",
      "\n",
      "# TODO what are the most common mistakes?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Bonus: Return to a previous lab \n",
      "# and replace its classifier (or regressor) with a neural net."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}